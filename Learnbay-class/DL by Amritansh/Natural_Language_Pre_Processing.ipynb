{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-d8PNd4X_NA"
   },
   "source": [
    "![alt text](https://miro.medium.com/max/1600/1*p_zgFaUyb66IeyHsi15soA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPJSIxy9YHDT"
   },
   "source": [
    "**NLP** is short for **Natural Language Processing**. As you probably know, computers are not as great at understanding words as they are numbers. This is all changing though as advances in NLP are happening everyday. The fact that devices like Apple’s Siri and Amazon’s Alexa can (usually) comprehend when we ask the weather, for directions, or to play a certain genre of music are all examples of NLP. The spam filter in your email and the spellcheck you’ve used since you learned to type in elementary school are some other basic examples of when your computer is understanding language.\n",
    "\n",
    "\n",
    "As a data scientist, we may use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things. Typically, whether we’re given the data or have to scrape it, the text will be in its natural human format of sentences, paragraphs, tweets, etc. From there, before we can dig into analyzing, we will have to do some cleaning to break the text down into a format the computer can easily understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zleUuQnJc0eK"
   },
   "source": [
    "# NLTK (Natural Language Toolkit)\n",
    "\n",
    "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. Although NLTK has adapted to more than 38 languages at present.\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning and wrappers for industrial-strength NLP libraries.  NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIpt5iUxd2e7"
   },
   "source": [
    "## NLP Library\t\n",
    "\n",
    "**NLTK** :\tThis is one of the most usable and mother of all NLP libraries.\n",
    "\n",
    "**spaCy**:\tThis is completely optimized and highly accurate library widely used in deep learning\n",
    "\n",
    "**Stanford CoreNLP Python**:\tFor client-server based architecture this is a good library in NLTK. This is written in JAVA, but it provides modularity to use it in Python.\n",
    "\n",
    "**TextBlob**:\tThis is an NLP library which works in python2 and python3. This is used for processing textual data and provide mainly all type of operation in the form of API.\n",
    "\n",
    "**Gensim**:\tGenism is a robust open source NLP library support in python. This library is highly efficient and scalable.\n",
    "\n",
    "**Pattern**:\tIt is a light-weighted NLP module. This is generally used in Web-mining, crawling or such type of spidering task. \n",
    "\n",
    "**Polyglot**:\tFor massive multilingual applications, Polyglot is best suitable NLP library. Feature extraction in the way on Identity and Entity.\n",
    "\n",
    "**PyNLPl**:\tPyNLPI also was known as 'Pineapple' and supports Python. It provides a parser for many data format like FoLiA/Giza/Moses/ARPA/Timbl/CQL.\n",
    "\n",
    "**Vocabulary**:\tThis library is best to get Semantic type information from the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:19.097305Z",
     "start_time": "2021-08-07T18:16:13.029305Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oM787FOXrBa",
    "outputId": "7f9646c6-0225-44ac-952e-0d0bc14311f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/vijay/anaconda3/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: click in /home/vijay/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /home/vijay/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in /home/vijay/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /home/vijay/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:28.952349Z",
     "start_time": "2021-08-07T18:16:19.100439Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_FQQfXYizDA",
    "outputId": "6d3b923f-9dea-4410-e190-4f7fb6475d0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:28.968770Z",
     "start_time": "2021-08-07T18:16:28.959355Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbr-JNS0jGO-",
    "outputId": "1e756817-e0cc-48a2-ef86-175a16214bec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:29.051468Z",
     "start_time": "2021-08-07T18:16:28.972266Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ib9mnxTojboi",
    "outputId": "7826e1db-307a-4989-ecf6-62b94f449c51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('genesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:29.135103Z",
     "start_time": "2021-08-07T18:16:29.054321Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8CLkw2DkbZn",
    "outputId": "e8582634-b9da-4372-b804-459d1c228307"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:29.351350Z",
     "start_time": "2021-08-07T18:16:29.137348Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAap0A7vnEcJ",
    "outputId": "b80e1701-5ff8-4141-d5ed-b5c0ba39870b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Leaves', 'of', 'Grass', 'by', 'Walt', 'Whitman', ...]\n"
     ]
    }
   ],
   "source": [
    "whitman = nltk.corpus.gutenberg.words('whitman-leaves.txt')\n",
    "print(whitman)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:29.434913Z",
     "start_time": "2021-08-07T18:16:29.354069Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_XUxeVZSDjb",
    "outputId": "803d6dee-8f49-467f-df86-aa74af5637fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:29.512688Z",
     "start_time": "2021-08-07T18:16:29.437993Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGjOPJgKSIrL",
    "outputId": "45773a94-5017-4ea2-8e34-ab5ca7f55538"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('nps_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:29.586340Z",
     "start_time": "2021-08-07T18:16:29.518580Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4szFVoWySOD7",
    "outputId": "d7db7502-a6b8-4e09-f7e7-f65109bbd851"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:29.661529Z",
     "start_time": "2021-08-07T18:16:29.590277Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9RzdCIUfSWW_",
    "outputId": "726ad3bd-912e-43ba-8ca9-8afc72b0bb48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:38.665664Z",
     "start_time": "2021-08-07T18:16:29.664250Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TP6t3bTbR2gm",
    "outputId": "dbaf13e9-97ab-4595-84d9-77a6c09cd8b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:38.675816Z",
     "start_time": "2021-08-07T18:16:38.669103Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4onQ6zaTJ9e",
    "outputId": "89493cb0-91aa-49da-872f-edb629185ada"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:38.735084Z",
     "start_time": "2021-08-07T18:16:38.677919Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-0uhXVxTybG",
    "outputId": "d3ab7a9c-ad73-4259-8710-c90ab214de44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16967"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:38.818126Z",
     "start_time": "2021-08-07T18:16:38.737818Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udAOrIf3T8yQ",
    "outputId": "787eface-6a1c-4262-d72a-332cae477839"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2166"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:38.927571Z",
     "start_time": "2021-08-07T18:16:38.821242Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pta8PteULH2",
    "outputId": "b81b6dde-0a36-493e-9270-5d670af1f9ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '!)',\n",
       " '!,',\n",
       " '!]',\n",
       " '#',\n",
       " \"'\",\n",
       " \"'!\",\n",
       " \"',\",\n",
       " \"'.\",\n",
       " \"'...\",\n",
       " \"'?\",\n",
       " '(',\n",
       " ',',\n",
       " \",'\",\n",
       " ',--',\n",
       " '-',\n",
       " '--',\n",
       " '--...',\n",
       " '.',\n",
       " \".'\",\n",
       " '.)',\n",
       " '..',\n",
       " '...',\n",
       " '...?',\n",
       " '...]',\n",
       " '1',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '2',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '?!',\n",
       " 'A',\n",
       " 'ALL',\n",
       " 'AMAZING',\n",
       " 'ANIMATOR',\n",
       " 'ARMY',\n",
       " 'ARTHUR',\n",
       " 'Aaaaaaaaah',\n",
       " 'Aaaaaaaah',\n",
       " 'Aaaaaah',\n",
       " 'Aaaah',\n",
       " 'Aaaaugh',\n",
       " 'Aaagh',\n",
       " 'Aaah',\n",
       " 'Aaauggh',\n",
       " 'Aaaugh',\n",
       " 'Aaauugh',\n",
       " 'Aagh',\n",
       " 'Aah',\n",
       " 'Aauuggghhh',\n",
       " 'Aauuugh',\n",
       " 'Aauuuuugh',\n",
       " 'Aauuuves',\n",
       " 'Action',\n",
       " 'Actually',\n",
       " 'African',\n",
       " 'Ages',\n",
       " 'Aggh',\n",
       " 'Agh',\n",
       " 'Ah',\n",
       " 'Ahh',\n",
       " 'Alice',\n",
       " 'All',\n",
       " 'Allo',\n",
       " 'Almighty',\n",
       " 'Alright',\n",
       " 'Am',\n",
       " 'Amen',\n",
       " 'An',\n",
       " 'Anarcho',\n",
       " 'And',\n",
       " 'Angnor',\n",
       " 'Anthrax',\n",
       " 'Antioch',\n",
       " 'Anybody',\n",
       " 'Anyway',\n",
       " 'Apples',\n",
       " 'Aramaic',\n",
       " 'Are',\n",
       " 'Arimathea',\n",
       " 'Armaments',\n",
       " 'Arthur',\n",
       " 'As',\n",
       " 'Ask',\n",
       " 'Assyria',\n",
       " 'At',\n",
       " 'Attila',\n",
       " 'Augh',\n",
       " 'Autumn',\n",
       " 'Auuuuuuuugh',\n",
       " 'Away',\n",
       " 'Ay',\n",
       " 'Ayy',\n",
       " 'B',\n",
       " 'BEDEVERE',\n",
       " 'BLACK',\n",
       " 'BORS',\n",
       " 'BRIDE',\n",
       " 'BRIDGEKEEPER',\n",
       " 'BROTHER',\n",
       " 'Back',\n",
       " 'Bad',\n",
       " 'Badon',\n",
       " 'Battle',\n",
       " 'Be',\n",
       " 'Beast',\n",
       " 'Bedevere',\n",
       " 'Bedwere',\n",
       " 'Behold',\n",
       " 'Between',\n",
       " 'Beyond',\n",
       " 'Black',\n",
       " 'Bloody',\n",
       " 'Blue',\n",
       " 'Bon',\n",
       " 'Bones',\n",
       " 'Book',\n",
       " 'Bors',\n",
       " 'Brave',\n",
       " 'Bravely',\n",
       " 'Bravest',\n",
       " 'Bread',\n",
       " 'Bridge',\n",
       " 'Bring',\n",
       " 'Bristol',\n",
       " 'Britain',\n",
       " 'Britons',\n",
       " 'Brother',\n",
       " 'Build',\n",
       " 'Burn',\n",
       " 'But',\n",
       " 'By',\n",
       " 'C',\n",
       " 'CAMERAMAN',\n",
       " 'CART',\n",
       " 'CARTOON',\n",
       " 'CHARACTER',\n",
       " 'CHARACTERS',\n",
       " 'CONCORDE',\n",
       " 'CRAPPER',\n",
       " 'CRASH',\n",
       " 'CRONE',\n",
       " 'CROWD',\n",
       " 'CUSTOMER',\n",
       " 'Caerbannog',\n",
       " 'Camaaaaaargue',\n",
       " 'Camelot',\n",
       " 'Castle',\n",
       " 'Chapter',\n",
       " 'Charge',\n",
       " 'Chaste',\n",
       " 'Cherries',\n",
       " 'Chicken',\n",
       " 'Chickennn',\n",
       " 'Chop',\n",
       " 'Christ',\n",
       " 'Churches',\n",
       " 'Cider',\n",
       " 'Clark',\n",
       " 'Clear',\n",
       " 'Come',\n",
       " 'Concorde',\n",
       " 'Consult',\n",
       " 'Cornwall',\n",
       " 'Could',\n",
       " 'Course',\n",
       " 'Court',\n",
       " 'Crapper',\n",
       " 'Cut',\n",
       " 'DEAD',\n",
       " 'DENNIS',\n",
       " 'DINGO',\n",
       " 'DIRECTOR',\n",
       " 'Dappy',\n",
       " 'Death',\n",
       " 'Defeat',\n",
       " 'Dennis',\n",
       " 'Did',\n",
       " 'Didn',\n",
       " 'Dingo',\n",
       " 'Dis',\n",
       " 'Divine',\n",
       " 'Do',\n",
       " 'Doctor',\n",
       " 'Does',\n",
       " 'Don',\n",
       " 'Dragon',\n",
       " 'Dramatically',\n",
       " 'ENCHANTER',\n",
       " 'Ecky',\n",
       " 'Ector',\n",
       " 'Eee',\n",
       " 'Eh',\n",
       " 'Enchanter',\n",
       " 'England',\n",
       " 'English',\n",
       " 'Erbert',\n",
       " 'Ere',\n",
       " 'Erm',\n",
       " 'Eternal',\n",
       " 'European',\n",
       " 'Even',\n",
       " 'Every',\n",
       " 'Everything',\n",
       " 'Ewing',\n",
       " 'Exactly',\n",
       " 'Excalibur',\n",
       " 'Excuse',\n",
       " 'Explain',\n",
       " 'FATHER',\n",
       " 'FRENCH',\n",
       " 'Far',\n",
       " 'Farewell',\n",
       " 'Father',\n",
       " 'Fetchez',\n",
       " 'Fiends',\n",
       " 'Fine',\n",
       " 'First',\n",
       " 'Firstly',\n",
       " 'Five',\n",
       " 'Follow',\n",
       " 'For',\n",
       " 'Forgive',\n",
       " 'Forward',\n",
       " 'Found',\n",
       " 'Four',\n",
       " 'France',\n",
       " 'Frank',\n",
       " 'French',\n",
       " 'GALAHAD',\n",
       " 'GIRLS',\n",
       " 'GOD',\n",
       " 'GREEN',\n",
       " 'GUARD',\n",
       " 'GUARDS',\n",
       " 'GUEST',\n",
       " 'GUESTS',\n",
       " 'Gable',\n",
       " 'Galahad',\n",
       " 'Gallahad',\n",
       " 'Gawain',\n",
       " 'Get',\n",
       " 'Go',\n",
       " 'God',\n",
       " 'Good',\n",
       " 'Gorge',\n",
       " 'Grail',\n",
       " 'Great',\n",
       " 'Greetings',\n",
       " 'Grenade',\n",
       " 'Guards',\n",
       " 'Guy',\n",
       " 'HEAD',\n",
       " 'HEADS',\n",
       " 'HERBERT',\n",
       " 'HISTORIAN',\n",
       " 'Ha',\n",
       " 'Hah',\n",
       " 'Hallo',\n",
       " 'Halt',\n",
       " 'Hand',\n",
       " 'Hang',\n",
       " 'Have',\n",
       " 'Haw',\n",
       " 'He',\n",
       " 'Hee',\n",
       " 'Heee',\n",
       " 'Heh',\n",
       " 'Hello',\n",
       " 'Help',\n",
       " 'Herbert',\n",
       " 'Here',\n",
       " 'Hey',\n",
       " 'Hic',\n",
       " 'Hill',\n",
       " 'Himself',\n",
       " 'His',\n",
       " 'Hiyaah',\n",
       " 'Hiyah',\n",
       " 'Hiyya',\n",
       " 'Hm',\n",
       " 'Hmm',\n",
       " 'Ho',\n",
       " 'Hoa',\n",
       " 'Hold',\n",
       " 'Holy',\n",
       " 'Honestly',\n",
       " 'Hoo',\n",
       " 'Hooray',\n",
       " 'How',\n",
       " 'Huh',\n",
       " 'Hurry',\n",
       " 'Huy',\n",
       " 'Huyah',\n",
       " 'Hya',\n",
       " 'Hyy',\n",
       " 'I',\n",
       " 'INSPECTOR',\n",
       " 'Idiom',\n",
       " 'Iesu',\n",
       " 'If',\n",
       " 'Iiiiives',\n",
       " 'Iiiives',\n",
       " 'In',\n",
       " 'Is',\n",
       " 'Isn',\n",
       " 'It',\n",
       " 'Ives',\n",
       " 'Jesus',\n",
       " 'Joseph',\n",
       " 'Just',\n",
       " 'KING',\n",
       " 'KNIGHT',\n",
       " 'KNIGHTS',\n",
       " 'Keep',\n",
       " 'King',\n",
       " 'Knight',\n",
       " 'Knights',\n",
       " 'LAUNCELOT',\n",
       " 'LEFT',\n",
       " 'LOVELY',\n",
       " 'LUCKY',\n",
       " 'Lady',\n",
       " 'Lake',\n",
       " 'Lancelot',\n",
       " 'Launcelot',\n",
       " 'Lead',\n",
       " 'Leaving',\n",
       " 'Let',\n",
       " 'Lie',\n",
       " 'Like',\n",
       " 'Listen',\n",
       " 'Loimbard',\n",
       " 'Look',\n",
       " 'Looks',\n",
       " 'Lord',\n",
       " 'Lucky',\n",
       " 'MAN',\n",
       " 'MASTER',\n",
       " 'MAYNARD',\n",
       " 'MIDDLE',\n",
       " 'MIDGET',\n",
       " 'MINSTREL',\n",
       " 'MONKS',\n",
       " 'Make',\n",
       " 'Man',\n",
       " 'May',\n",
       " 'Maynard',\n",
       " 'Meanwhile',\n",
       " 'Mercea',\n",
       " 'Message',\n",
       " 'Midget',\n",
       " 'Mind',\n",
       " 'Mine',\n",
       " 'Mmm',\n",
       " 'Monsieur',\n",
       " 'More',\n",
       " 'Morning',\n",
       " 'Most',\n",
       " 'Mother',\n",
       " 'Mud',\n",
       " 'Must',\n",
       " 'My',\n",
       " 'N',\n",
       " 'NARRATOR',\n",
       " 'NI',\n",
       " 'Nador',\n",
       " 'Nay',\n",
       " 'Neee',\n",
       " 'Never',\n",
       " 'Ni',\n",
       " 'Nine',\n",
       " 'Ninepence',\n",
       " 'No',\n",
       " 'None',\n",
       " 'Not',\n",
       " 'Nothing',\n",
       " 'Now',\n",
       " 'Nu',\n",
       " 'O',\n",
       " 'OF',\n",
       " 'OFFICER',\n",
       " 'OLD',\n",
       " 'OTHER',\n",
       " 'Of',\n",
       " 'Off',\n",
       " 'Oh',\n",
       " 'Ohh',\n",
       " 'Old',\n",
       " 'Olfin',\n",
       " 'On',\n",
       " 'Once',\n",
       " 'One',\n",
       " 'Ooh',\n",
       " 'Oooh',\n",
       " 'Oooo',\n",
       " 'Oooohoohohooo',\n",
       " 'Oooooooh',\n",
       " 'Open',\n",
       " 'Or',\n",
       " 'Order',\n",
       " 'Other',\n",
       " 'Oui',\n",
       " 'Our',\n",
       " 'Over',\n",
       " 'Ow',\n",
       " 'PARTY',\n",
       " 'PATSY',\n",
       " 'PERSON',\n",
       " 'PIGLET',\n",
       " 'PRINCE',\n",
       " 'PRINCESS',\n",
       " 'PRISONER',\n",
       " 'Packing',\n",
       " 'Patsy',\n",
       " 'Pendragon',\n",
       " 'Peng',\n",
       " 'Perhaps',\n",
       " 'Peril',\n",
       " 'Picture',\n",
       " 'Pie',\n",
       " 'Piglet',\n",
       " 'Pin',\n",
       " 'Please',\n",
       " 'Practice',\n",
       " 'Prepare',\n",
       " 'Prince',\n",
       " 'Princess',\n",
       " 'Providence',\n",
       " 'Psalms',\n",
       " 'Pull',\n",
       " 'Pure',\n",
       " 'Put',\n",
       " 'Quick',\n",
       " 'Quickly',\n",
       " 'Quiet',\n",
       " 'Quite',\n",
       " 'Quoi',\n",
       " 'RANDOM',\n",
       " 'RIGHT',\n",
       " 'ROBIN',\n",
       " 'ROGER',\n",
       " 'Rather',\n",
       " 'Really',\n",
       " 'Recently',\n",
       " 'Remove',\n",
       " 'Rheged',\n",
       " 'Ridden',\n",
       " 'Right',\n",
       " 'Riiight',\n",
       " 'Robin',\n",
       " 'Robinson',\n",
       " 'Roger',\n",
       " 'Round',\n",
       " 'Run',\n",
       " 'Running',\n",
       " 'S',\n",
       " 'SCENE',\n",
       " 'SECOND',\n",
       " 'SENTRY',\n",
       " 'SHRUBBER',\n",
       " 'SIR',\n",
       " 'SOLDIER',\n",
       " 'STUNNER',\n",
       " 'SUN',\n",
       " 'Said',\n",
       " 'Saint',\n",
       " 'Saxons',\n",
       " 'Say',\n",
       " 'Schools',\n",
       " 'See',\n",
       " 'Seek',\n",
       " 'Shall',\n",
       " 'She',\n",
       " 'Shh',\n",
       " 'Shrubber',\n",
       " 'Shrubberies',\n",
       " 'Shut',\n",
       " 'Silence',\n",
       " 'Silly',\n",
       " 'Since',\n",
       " 'Sir',\n",
       " 'Skip',\n",
       " 'So',\n",
       " 'Sorry',\n",
       " 'Speak',\n",
       " 'Splendid',\n",
       " 'Spring',\n",
       " 'Stand',\n",
       " 'Stay',\n",
       " 'Steady',\n",
       " 'Stop',\n",
       " 'Summer',\n",
       " 'Supposing',\n",
       " 'Supreme',\n",
       " 'Surely',\n",
       " 'Swamp',\n",
       " 'THE',\n",
       " 'TIM',\n",
       " 'Table',\n",
       " 'Tale',\n",
       " 'Tall',\n",
       " 'Tell',\n",
       " 'Thank',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Thee',\n",
       " 'Then',\n",
       " 'There',\n",
       " 'Therefore',\n",
       " 'They',\n",
       " 'This',\n",
       " 'Those',\n",
       " 'Thou',\n",
       " 'Thpppppt',\n",
       " 'Thppppt',\n",
       " 'Thpppt',\n",
       " 'Thppt',\n",
       " 'Three',\n",
       " 'Throw',\n",
       " 'Thsss',\n",
       " 'Thursday',\n",
       " 'Thy',\n",
       " 'Til',\n",
       " 'Tim',\n",
       " 'Tis',\n",
       " 'To',\n",
       " 'Today',\n",
       " 'Together',\n",
       " 'Too',\n",
       " 'Torment',\n",
       " 'Tower',\n",
       " 'True',\n",
       " 'Try',\n",
       " 'Twenty',\n",
       " 'Two',\n",
       " 'U',\n",
       " 'Uh',\n",
       " 'Uhh',\n",
       " 'Ulk',\n",
       " 'Um',\n",
       " 'Umhm',\n",
       " 'Umm',\n",
       " 'Un',\n",
       " 'Unfortunately',\n",
       " 'Until',\n",
       " 'Use',\n",
       " 'Uther',\n",
       " 'Uugh',\n",
       " 'Uuh',\n",
       " 'VILLAGER',\n",
       " 'VILLAGERS',\n",
       " 'VOICE',\n",
       " 'Very',\n",
       " 'Victory',\n",
       " 'W',\n",
       " 'WIFE',\n",
       " 'WINSTON',\n",
       " 'WITCH',\n",
       " 'WOMAN',\n",
       " 'Waa',\n",
       " 'Wait',\n",
       " 'Walk',\n",
       " 'Wayy',\n",
       " 'We',\n",
       " 'Welcome',\n",
       " 'Well',\n",
       " 'What',\n",
       " 'When',\n",
       " 'Where',\n",
       " 'Which',\n",
       " 'Who',\n",
       " 'Whoa',\n",
       " 'Why',\n",
       " 'Will',\n",
       " 'Winston',\n",
       " 'Winter',\n",
       " 'With',\n",
       " 'Woa',\n",
       " 'Wood',\n",
       " 'Would',\n",
       " 'Y',\n",
       " 'Yapping',\n",
       " 'Yay',\n",
       " 'Yeaaah',\n",
       " 'Yeaah',\n",
       " 'Yeah',\n",
       " 'Yes',\n",
       " 'You',\n",
       " 'Your',\n",
       " 'Yup',\n",
       " 'ZOOT',\n",
       " 'Zoot',\n",
       " '[',\n",
       " '[...',\n",
       " ']',\n",
       " 'a',\n",
       " 'aaaaaah',\n",
       " 'aaaah',\n",
       " 'aaggggh',\n",
       " 'aaugh',\n",
       " 'able',\n",
       " 'about',\n",
       " 'absolutely',\n",
       " 'accent',\n",
       " 'accompanied',\n",
       " 'accomplished',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'actually',\n",
       " 'advancing',\n",
       " 'adversary',\n",
       " 'affairs',\n",
       " 'afoot',\n",
       " 'afraid',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agree',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'alarm',\n",
       " 'alight',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allowed',\n",
       " 'almost',\n",
       " 'aloft',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazes',\n",
       " 'an',\n",
       " 'anarcho',\n",
       " 'anchovies',\n",
       " 'and',\n",
       " 'angels',\n",
       " 'anging',\n",
       " 'animal',\n",
       " 'animator',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apologise',\n",
       " 'appearing',\n",
       " 'appease',\n",
       " 'approacheth',\n",
       " 'approaching',\n",
       " 'aptly',\n",
       " 'aquatic',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'argue',\n",
       " 'arm',\n",
       " 'armed',\n",
       " 'armor',\n",
       " 'arms',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arrows',\n",
       " 'art',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'assault',\n",
       " 'assist',\n",
       " 'at',\n",
       " 'attack',\n",
       " 'attend',\n",
       " 'auntie',\n",
       " 'aunties',\n",
       " 'autocracy',\n",
       " 'automatically',\n",
       " 'autonomous',\n",
       " 'auuuuuuuugh',\n",
       " 'avenged',\n",
       " 'averting',\n",
       " 'awaaaaay',\n",
       " 'awaaay',\n",
       " 'awaits',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'b',\n",
       " 'baaaa',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'badger',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bang',\n",
       " 'bangin',\n",
       " 'basic',\n",
       " 'basis',\n",
       " 'bastard',\n",
       " 'bastards',\n",
       " 'bathing',\n",
       " 'bats',\n",
       " 'be',\n",
       " 'beacon',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'beds',\n",
       " 'been',\n",
       " 'behaviour',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'being',\n",
       " 'bells',\n",
       " 'bent',\n",
       " 'beside',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bi',\n",
       " 'bicker',\n",
       " 'bid',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'binding',\n",
       " 'bint',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'biscuits',\n",
       " 'bit',\n",
       " 'bitching',\n",
       " 'bite',\n",
       " 'biters',\n",
       " 'bits',\n",
       " 'bladders',\n",
       " 'blanket',\n",
       " 'bleed',\n",
       " 'bleeder',\n",
       " 'bless',\n",
       " 'blessing',\n",
       " 'blondes',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'body',\n",
       " 'boil',\n",
       " 'boing',\n",
       " 'bois',\n",
       " 'bold',\n",
       " 'bond',\n",
       " 'bones',\n",
       " 'bonk',\n",
       " 'boom',\n",
       " 'bosom',\n",
       " 'bother',\n",
       " 'bottom',\n",
       " 'bottoms',\n",
       " 'bowels',\n",
       " 'bows',\n",
       " 'boys',\n",
       " 'brain',\n",
       " 'brained',\n",
       " 'brave',\n",
       " 'bravely',\n",
       " 'bravest',\n",
       " 'breadth',\n",
       " 'breakfast',\n",
       " 'breath',\n",
       " 'bride',\n",
       " 'bridge',\n",
       " 'bridgekeeper',\n",
       " 'bridges',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'broken',\n",
       " 'brought',\n",
       " 'brunettes',\n",
       " 'brush',\n",
       " 'bugger',\n",
       " 'buggered',\n",
       " 'buggering',\n",
       " 'build',\n",
       " 'built',\n",
       " 'bum',\n",
       " 'bunny',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burst',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'c',\n",
       " 'cadeau',\n",
       " 'call',\n",
       " 'called',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'capital',\n",
       " 'carp',\n",
       " 'carried',\n",
       " 'carries',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'cart',\n",
       " 'cartoon',\n",
       " 'carve',\n",
       " 'carved',\n",
       " 'carving',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'castanets',\n",
       " 'castle',\n",
       " 'cause',\n",
       " 'cave',\n",
       " 'centuries',\n",
       " 'cereals',\n",
       " 'ceremony',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'chanting',\n",
       " 'charged',\n",
       " 'chastity',\n",
       " 'cheesy',\n",
       " 'chest',\n",
       " 'chickened',\n",
       " 'chickening',\n",
       " 'chops',\n",
       " 'chord',\n",
       " 'chorus',\n",
       " 'chosen',\n",
       " 'chu',\n",
       " 'clack',\n",
       " 'clad',\n",
       " 'clang',\n",
       " 'clank',\n",
       " 'clap',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'clear',\n",
       " 'clever',\n",
       " 'climes',\n",
       " 'clllank',\n",
       " 'clop',\n",
       " 'closest',\n",
       " 'clue',\n",
       " 'clunk',\n",
       " 'coconut',\n",
       " 'coconuts',\n",
       " 'collective',\n",
       " 'color',\n",
       " 'come',\n",
       " 'comin',\n",
       " 'coming',\n",
       " 'command',\n",
       " 'commands',\n",
       " 'committed',\n",
       " 'commune',\n",
       " 'compared',\n",
       " 'completely',\n",
       " 'conclusion',\n",
       " 'conclusions',\n",
       " 'confuse',\n",
       " 'considerable',\n",
       " 'consulted',\n",
       " 'continue',\n",
       " 'convinced',\n",
       " 'cop',\n",
       " 'cope',\n",
       " 'cost',\n",
       " 'cough',\n",
       " 'could',\n",
       " 'couldn',\n",
       " 'count',\n",
       " 'counting',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'courage',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'covered',\n",
       " 'crash',\n",
       " 'creak',\n",
       " 'creature',\n",
       " 'creep',\n",
       " 'creeper',\n",
       " 'crone',\n",
       " 'cross',\n",
       " 'crossed',\n",
       " 'cruel',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'curtains',\n",
       " 'cut',\n",
       " 'd',\n",
       " 'dad',\n",
       " 'daft',\n",
       " 'dance',\n",
       " 'dancing',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'dappy',\n",
       " 'dare',\n",
       " 'daring',\n",
       " 'dark',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'deeds',\n",
       " 'defeat',\n",
       " 'defeator',\n",
       " 'delirious',\n",
       " 'demand',\n",
       " 'depart',\n",
       " 'depressing',\n",
       " 'derives',\n",
       " 'design',\n",
       " 'diaphragm',\n",
       " 'dictating',\n",
       " 'dictatorship',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'died',\n",
       " 'differences',\n",
       " 'dine',\n",
       " 'direction',\n",
       " 'dirty',\n",
       " 'discovered',\n",
       " 'discovers',\n",
       " 'disheartened',\n",
       " 'distress',\n",
       " 'distributing',\n",
       " 'do',\n",
       " 'doctors',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'dogma',\n",
       " 'dogs',\n",
       " 'doing',\n",
       " 'domine',\n",
       " 'don',\n",
       " 'dona',\n",
       " 'donaeis',\n",
       " 'done',\n",
       " 'donkey',\n",
       " 'door',\n",
       " 'doors',\n",
       " 'dorsal',\n",
       " 'doubt',\n",
       " 'down',\n",
       " 'dragging',\n",
       " 'dramatic',\n",
       " 'draw',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(text6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bahL29ECsBFQ"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLTujHgRsEmh"
   },
   "source": [
    "We will talk about the basic steps of text preprocessing. These steps are needed for transferring text from human language to machine-readable format for further processing. We will also discuss text preprocessing tools.\n",
    "\n",
    "After a text is obtained, we start with text normalization. Text normalization includes:\n",
    "\n",
    "\n",
    "\n",
    "*    removing punctuations, accent marks and other diacritics\n",
    "*    removing white spaces\n",
    "*    expanding abbreviations\n",
    "*    removing stop words, sparse terms, and particular words\n",
    "*    text canonicalization\n",
    "*    converting all letters to lower or upper case\n",
    "*    converting numbers into words or removing numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjQkmTZqyUQx"
   },
   "source": [
    "### Removing punctuations, accent marks, special symbols and diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:38.990247Z",
     "start_time": "2021-08-07T18:16:38.929427Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "R7IMxZmryTxk",
    "outputId": "706c40cd-07a3-4d56-97c7-81fba178ebe2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from my given string object'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample code to remove a regex pattern \n",
    "import re \n",
    "\n",
    "def remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\"  \n",
    "\n",
    "remove_regex(\"remove this #hashtag from my given string object\", regex_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:39.115781Z",
     "start_time": "2021-08-07T18:16:38.992720Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j27MhDzWSbs8",
    "outputId": "b2b55470-7b30-4754-80b7-e13bd375be0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from my given string object'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_pattern = \"@[\\w]*\"  \n",
    "\n",
    "remove_regex(\"remove this @learnbay from my given string object\", regex_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dwlWziP4nZR"
   },
   "source": [
    "### Remove whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:39.188095Z",
     "start_time": "2021-08-07T18:16:39.119112Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "CTICTDEB41e0",
    "outputId": "c7f3fdcc-3566-4994-84e0-21e17569a07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t a string example\t \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a string example'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \" \\t a string example\\t \"\n",
    "print (input_str)\n",
    "input_str = input_str.strip()\n",
    "input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:39.250986Z",
     "start_time": "2021-08-07T18:16:39.190744Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUpB4Ubu2eVM",
    "outputId": "53cfd28f-e4ca-40c9-e139-2d7a2bee09d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'string', 'example']\n"
     ]
    }
   ],
   "source": [
    "print(input_str.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHPBW4PZ5Eqa"
   },
   "source": [
    "### Remove or Replace Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:39.312025Z",
     "start_time": "2021-08-07T18:16:39.254067Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BiwsgsiTatD9",
    "outputId": "66e707da-53ee-476f-db34-2831b2903ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' banana  apple  oranges'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_pattern = \"[0-9]+\"  \n",
    "input_txt = '1 banana 23 apple 456 oranges'\n",
    "\n",
    "remove_regex(input_txt, regex_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:39.376947Z",
     "start_time": "2021-08-07T18:16:39.313659Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4A18T2f5G-E",
    "outputId": "ace3f5a0-0c5a-4c81-f6ba-14cfb8b4b7ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "substitution_dict = {3:\"three\",5:\"five\"}\n",
    "input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n",
    "result = re.sub(r\"\\d+\", '', input_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:42.200765Z",
     "start_time": "2021-08-07T18:16:39.381448Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjnZ1Km2goh1",
    "outputId": "9501a219-a28f-44c5-d79d-f329c371db10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in /home/vijay/anaconda3/lib/python3.8/site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /home/vijay/anaconda3/lib/python3.8/site-packages (from num2words) (0.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:42.522105Z",
     "start_time": "2021-08-07T18:16:42.204285Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wXDsxW99gq7x",
    "outputId": "cdebc7a9-7cb8-4b13-9255-71b3f2c92efa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forty-two'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from num2words import num2words\n",
    "num2words(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:42.529072Z",
     "start_time": "2021-08-07T18:16:42.523890Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiWSuO0oVbn2",
    "outputId": "1817c69b-b274-450e-f833-0e67b42cddd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains three red and five white balls, while Box B contains four red and two blue balls.\n"
     ]
    }
   ],
   "source": [
    "input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n",
    "out = ' '.join([num2words(i) if i.isdigit() else i for i in input_str.split()])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:42.596479Z",
     "start_time": "2021-08-07T18:16:42.531304Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "qCM_MOeqg8s9",
    "outputId": "c88579cb-c696-4958-c401-4ec4abadb2f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Box A contains  red and  white balls, while Box B contains  red and  blue balls.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homework\n",
    "# swap all the number to their equivalent words using regex only\n",
    "import re\n",
    "input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n",
    "result = re.sub(r\"\\d+\", '' , input_str)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xo4yDnL5c7x"
   },
   "source": [
    "### Convert Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:42.669859Z",
     "start_time": "2021-08-07T18:16:42.600120Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PeQIa5xK5gV3",
    "outputId": "9dbc521d-b5c6-4bcd-d1ae-2ef4a2ab5b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
    "input_str = input_str.lower()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3nA6fDk5toc"
   },
   "source": [
    "**Tokenization**\n",
    "\n",
    "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:42.738291Z",
     "start_time": "2021-08-07T18:16:42.678335Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrQ32n096_GA",
    "outputId": "de0e648a-8e87-43c2-ff8c-6733983f652c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:42.886339Z",
     "start_time": "2021-08-07T18:16:42.741238Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAhXTfte5wPN",
    "outputId": "fd44a49c-6f7f-438b-885c-7294f0d7f53e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(input_str)\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhq0zde57Itt"
   },
   "source": [
    "### Remove stop words\n",
    "\n",
    "“Stop words” are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:42.904971Z",
     "start_time": "2021-08-07T18:16:42.889244Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oD8Gnon73KT",
    "outputId": "2695dc65-f640-46ff-ab43-d152c98f93ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:43.094461Z",
     "start_time": "2021-08-07T18:16:42.907737Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YIeqnHZ7PUe",
    "outputId": "e27f70f0-c897-464e-fe56-b5a89ea86447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"hasn't\", \"mightn't\", 'hers', 'myself', 'does', 'while', 'been', 'both', 'some', 'ma', 'during', 'no', 'through', 'mustn', 'haven', 'her', 'above', 'have', 'has', 'what', 'theirs', \"shouldn't\", \"won't\", 'the', 'be', 'hasn', 'mightn', 'll', 't', 'our', 'shan', 'below', 'me', 'out', \"aren't\", 'wasn', 'do', 'here', 'needn', 're', 'i', 'who', \"weren't\", 'd', 'couldn', 'down', 'this', 'o', 'over', 'about', 'only', \"mustn't\", \"should've\", 'him', 'than', 'doing', \"shan't\", 'isn', 'y', 'don', 'so', 'himself', 'few', 'shouldn', 'them', 'but', 'they', 'm', 'an', 'under', 'more', 'of', 'nor', 'then', 'that', 'from', 'too', 'other', 'by', 'up', 'my', \"you're\", 'yourselves', 'these', 'herself', 'or', 'with', \"couldn't\", 'did', 'on', 'didn', 'hadn', 'won', 'were', 'we', 'just', \"don't\", 's', \"it's\", 'aren', 'she', 'ours', 'yours', 'in', 'off', 'yourself', 'why', 'as', 'themselves', 'ain', 'when', 'any', 'at', \"you'll\", \"isn't\", 'had', 'same', 'being', 'your', 'such', 'own', 'ourselves', 'his', 'all', 'their', 'wouldn', 'can', 'itself', \"haven't\", 'weren', 'and', 'will', \"you'd\", \"needn't\", 'was', 'there', 'against', 'most', 'should', 'until', 'if', 'further', 'doesn', 'for', 'after', 've', \"you've\", 'is', 'not', 'are', 'each', \"she's\", \"doesn't\", 'once', 'its', 'those', 'again', 'he', 'a', 'you', \"didn't\", \"that'll\", 'between', 'into', \"hadn't\", 'whom', 'to', 'which', 'now', \"wasn't\", 'very', 'it', 'how', 'having', \"wouldn't\", 'because', 'where', 'before', 'am'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:43.166197Z",
     "start_time": "2021-08-07T18:16:43.097518Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CP3gK7j7-FL",
    "outputId": "ff3e2284-6404-4982-e1a7-609a618e3293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'Its', 'good', 'go', 'fun', 'times', '.']\n"
     ]
    }
   ],
   "source": [
    "input_str = \"All work and no play makes jack a dull boy. Its good to go out and have fun at times.\"\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:43.249020Z",
     "start_time": "2021-08-07T18:16:43.168901Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvCGoQyH8YSX",
    "outputId": "d8dd731e-0835-46ee-e588-1616fda9de00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'cant', 'twenty', 'myself', 'name', 'both', 'first', 'amongst', 'whose', 'her', 'above', 'anyhow', 'sometimes', 'thereby', 'interest', 'our', 'latterly', 'bill', 'do', 'none', 'who', 'seem', 'mine', 'whether', 'so', 'few', 'himself', 'nothing', 'noone', 'of', 'that', 'towards', 'become', 'might', 'on', 'eg', 'twelve', 'upon', 'ours', 'whence', 'side', 'always', 'ever', 'describe', 'same', 'somehow', 'per', 'empty', 'six', 'nevertheless', 'back', 'itself', 'wherever', 'anything', 'will', 'moreover', 'two', 'someone', 'should', 'if', 'part', 'still', 'after', 'again', 'its', 'therefore', 'rather', 'serious', 'hasnt', 'he', 'neither', 'hereafter', 'to', 'which', 'hereupon', 'beside', 'it', 'latter', 'whereafter', 'once', 'before', 'am', 'hence', 'un', 'while', 'every', 'has', 'what', 'already', 'be', 'below', 'i', 'behind', 'sixty', 'three', 'never', 'fifteen', 'whenever', 'whole', 'fifty', 'ten', 'them', 'but', 'done', 'made', 'more', 'anywhere', 'five', 'nor', 'call', 'then', 'by', 'find', 'indeed', 'my', 'these', 'couldnt', 'one', 'whereas', 'around', 'yours', 'thin', 'thereafter', 'why', 'yourself', 'found', 'whereby', 'themselves', 'any', 'except', 'third', 'seems', 'whereupon', 'being', 'ourselves', 'all', 'somewhere', 'must', 'though', 'onto', 'without', 'and', 'elsewhere', 'becoming', 'almost', 'became', 'cannot', 'de', 'until', 'among', 'are', 'those', 'enough', 'however', 'between', 'ie', 'thence', 'very', 'amoungst', 'sincere', 'wherein', 'please', 'across', 'eleven', 'thereupon', 'toward', 'nine', 'give', 'been', 'full', 'some', 'during', 'no', 'through', 'less', 'the', 'namely', 'me', 'here', 're', 'down', 'about', 'him', 'many', 'forty', 'even', 'whither', 'from', 'another', 'otherwise', 'beyond', 'system', 'everywhere', 'were', 'she', 'as', 'else', 'whatever', 'when', 'at', 'front', 'amount', 'fire', 'his', 'their', 'fill', 'everyone', 'former', 'least', 'alone', 'hereby', 'detail', 'eight', 'although', 'others', 'was', 'herein', 'against', 'top', 'is', 'thick', 'much', 'a', 'you', 'into', 'becomes', 'whom', 'thru', 'now', 'cry', 'beforehand', 'where', 'anyway', 'hers', 'co', 'via', 'formerly', 'throughout', 'take', 'either', 'yet', 'have', 'within', 'get', 'nobody', 'out', 'seemed', 'whoever', 'afterwards', 'four', 'inc', 'this', 'over', 'only', 'something', 'than', 'together', 'keep', 'they', 'an', 'since', 'nowhere', 'under', 'due', 'besides', 'show', 'too', 'other', 'up', 'often', 'yourselves', 'herself', 'or', 'with', 'we', 'would', 'along', 'go', 'off', 'in', 'bottom', 'therein', 'had', 'may', 'such', 'well', 'own', 'your', 'see', 'can', 'put', 'thus', 'meanwhile', 'hundred', 'perhaps', 'etc', 'anyone', 'there', 'most', 'further', 'seeming', 'next', 'for', 'also', 'us', 'mill', 'not', 'each', 'several', 'last', 'everything', 'con', 'ltd', 'mostly', 'move', 'how', 'sometime', 'could', 'because'})\n"
     ]
    }
   ],
   "source": [
    "#sklearn can also provide a list of standard english stop words\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print (ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCT4lmDA8oPc"
   },
   "source": [
    "Most of what we are going to do with language relies on ﬁrst separating out or tokenizing words (splitting the text into minimal meaningful units) from running text, known as the task of tokenization.\n",
    "\n",
    "English words are often separated from each other by whitespace, but whitespace is not always sufﬁcient. “New York” and “rock ’n’ roll” are sometimes treated as large words despite the fact that they contain spaces, while sometimes we’ll need to separate “I’m” into the two words I and am.\n",
    "\n",
    "For processing tweets or texts we’ll need to tokenize emoticons like “ :)” or hashtags like #nlproc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEDpRmdek2tG"
   },
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n",
    "\n",
    "Stemming is a part of linguistic studies in morphology and artificial intelligence (AI) information retrieval and extraction. Stemming and AI knowledge extract meaningful information from vast sources like big data or the Internet since additional forms of a word related to a subject may need to be searched to get the best results. Stemming is also a part of queries and Internet search engines.\n",
    "\n",
    "Recognizing, searching and retrieving more forms of words returns more results. When a form of a word is recognized it can make it possible to return search results that otherwise might have been missed. That additional information retrieved is why stemming is integral to search queries and information retrieval.\n",
    "\n",
    "\n",
    "Applications of stemming are:\n",
    "\n",
    "* Stemming is used in information retrieval systems like search engines.\n",
    "* It is used to determine domain vocabularies in domain analysis.\n",
    "* Stemming is desirable as it may reduce redundancy as most of the time the word stem and their inflected/derived words mean the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:43.316203Z",
     "start_time": "2021-08-07T18:16:43.252284Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81cWs5E0mySu",
    "outputId": "6d29f8b2-db3f-43ee-aacf-3bb1a3266e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      "for\n",
      "natur\n",
      "languag\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "input_str=\"There are several types of stemming algorithms for Natural languages\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqHDNKw9nbNN"
   },
   "source": [
    "**Errors in Stemming**:\n",
    "There are mainly two errors in stemming – Overstemming and Understemming. Overstemming occurs when two words are stemmed to same root that are of different stems. Under-stemming occurs when two words are stemmed to same root that are not of different stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:43.389619Z",
     "start_time": "2021-08-07T18:16:43.318468Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0zuqyFen0Yl",
    "outputId": "be26dca1-55ce-4223-c53b-ffaa53080b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\n",
      "having\n",
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "print(stemmer.stem(\"having\"))\n",
    "print(stemmer2.stem(\"having\"))\n",
    "\n",
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:43.466295Z",
     "start_time": "2021-08-07T18:16:43.393228Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLmxixHh4UDQ",
    "outputId": "a176535e-785f-4151-9b58-c48b445697b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "say\n",
      "today\n",
      "our\n",
      "player\n",
      "were\n",
      "not\n",
      "play\n",
      "with\n",
      "the\n",
      "spirit\n",
      "of\n",
      "the\n",
      "game\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# input_str=\"There are several types of stemming algorithms for Natural languages\"\n",
    "input_str = \"We say today our players were not playing with the spirit of the game.\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmzAmVUQuvIC"
   },
   "source": [
    "**N-Gram Stemmer**\n",
    "\n",
    "An n-gram is a set of n consecutive characters extracted from a word in which similar words will have a high proportion of n-grams in common.\n",
    "Example: ‘INTRODUCTIONS’ for n=2 becomes : *I, IN, NT, TR, RO, OD, DU, UC, CT, TI, IO, ON, NS, S*\n",
    "\n",
    "Advantage: It is based on string comparisons and it is language dependent.\n",
    "\n",
    "Limitation: It requires space to create and index the n-grams and it is not time efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHCYQ6GDvP4z"
   },
   "source": [
    "### Lemmatizer\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEYVaVrVv39A"
   },
   "source": [
    "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    "\n",
    "For instance:\n",
    "\n",
    "The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "\n",
    "The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    "\n",
    "The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:43.539109Z",
     "start_time": "2021-08-07T18:16:43.468883Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrTwcJaEvX0k",
    "outputId": "6bb0df09-ea80-46ee-8f4e-c1cce94cec02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:16:46.176473Z",
     "start_time": "2021-08-07T18:16:43.541566Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Lnqi93NnvR1U",
    "outputId": "0c3f56cc-e78b-49c6-e544-4270d23b7509"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'played'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "lemma.lemmatize('played')\n",
    "# lemma.lemmatize('meeting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:17:05.593851Z",
     "start_time": "2021-08-07T18:17:05.584888Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-pUL3So59YO",
    "outputId": "f7fc8853-6983-4772-a79d-ede331942dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There\n",
      "are\n",
      "several\n",
      "type\n",
      "of\n",
      "stemming\n",
      "algorithm\n",
      "for\n",
      "Natural\n",
      "language\n"
     ]
    }
   ],
   "source": [
    "input_str=\"There are several types of stemming algorithms for Natural languages\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemma.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re74ahJWwTf-"
   },
   "source": [
    "**Object Standardization**\n",
    "\n",
    "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n",
    "\n",
    "Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:17:42.235806Z",
     "start_time": "2021-08-07T18:17:42.227149Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mLgigvpwVeU",
    "outputId": "f9e3eb29-8816-441a-f23d-5ff194db5c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retweet We are going to CCD at MG Road!! direct message for more information !!\n"
     ]
    }
   ],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\", \"@\":\"at\", \"info\":\"information\"}\n",
    "def lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words) \n",
    "    return new_text\n",
    "\n",
    "print(lookup_words(\"RT We are going to CCD @ MG Road!! dm for more info !!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:17:44.626935Z",
     "start_time": "2021-08-07T18:17:44.620853Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1TQXmbG7M5l",
    "outputId": "de953367-5d54-42ad-fda9-2a394160ce5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IEEE 811.11ax': 'Wireless protocol for internet router and network cards'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"IEEE 811.11ax\": \"Wireless protocol for internet router and network cards\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Natural Language Pre - Processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
